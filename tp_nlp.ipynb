{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgvWgpoDfTNMFDDWC0wX9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agustin-Galarza/tp_nlp/blob/main/tp_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "d1hHf-GXpuJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Ek8dP5DO-Igh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "from typing import Dict, List, Optional, Iterable\n",
        "from pandas import DataFrame\n",
        "from enum import Enum\n",
        "import sys"
      ],
      "metadata": {
        "id": "EY4eLXqfppZq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get credentials to acces the drive folder"
      ],
      "metadata": {
        "id": "ovlYOc249dFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "Lw1yVbk4-Guk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read file and store all the parsed programs"
      ],
      "metadata": {
        "id": "LQ3WjTJU9ick"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_filename = \"Repos_TypeScriptJan2022_duplicates_removed3.jsonl\"\n",
        "\n",
        "datadir = \"./sample_data\"\n",
        "filename = \"test_data.jsonl\"\n",
        "\n",
        "# Download the file from Drive\n",
        "results = drive.ListFile({'q': f'title = \"{drive_filename}\"'}).GetList()\n",
        "if len(results) == 0:\n",
        "  raise Exception(\"No file found\")\n",
        "if len(results) > 1:\n",
        "  raise Exception(\"Too many results\")\n",
        "file = results[0]\n",
        "file.GetContentFile(f'{datadir}/{filename}')\n",
        "\n",
        "# Open the file and separate the files\n",
        "\n",
        "## Each file has a collection of repositories from Github\n",
        "REPOS_TO_SCAN = -1 # -1 to scan all repos\n",
        "FILES_PER_REPO = -1  # -1 to get all files from each selected repo\n",
        "\n",
        "files: List[List[str]] = []\n",
        "with open(f\"{datadir}/{filename}\") as dataset:\n",
        "    for line_no, line in enumerate(dataset):\n",
        "        if REPOS_TO_SCAN != -1 and line_no == REPOS_TO_SCAN:\n",
        "            break\n",
        "        repository = json.loads(line)\n",
        "        filesdata: Dict = repository.get(\"filedata\")\n",
        "        for i, file in enumerate(filesdata.values()):\n",
        "            if FILES_PER_REPO != -1 and i == FILES_PER_REPO:\n",
        "                break\n",
        "            tokens = file.get(\"tokens\")\n",
        "            files.append(tokens)"
      ],
      "metadata": {
        "id": "X7EOAMnGtNIr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function and type definitions"
      ],
      "metadata": {
        "id": "VNl7O38j91FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FunctionData:\n",
        "    name: str\n",
        "    params: List[str]\n",
        "    block: List[str]\n",
        "\n",
        "    def copy(self) -> \"FunctionData\":\n",
        "        return FunctionData(self.name[:], self.params[:], self.block[:])\n",
        "\n",
        "\n",
        "class ParametersParsingData:\n",
        "    is_type_def = False\n",
        "    braces: List[str] = []\n",
        "    sharp_braces: List[str] = []\n",
        "    parenthesis: List[str] = []\n",
        "\n",
        "\n",
        "class BlockParsingData:\n",
        "    braces: List[str] = []\n",
        "\n",
        "\n",
        "class ReturnTypeData:\n",
        "    containers: int = 0\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.tokens_len = len(tokens)\n",
        "        self.current_index = 0\n",
        "        self.marker = 0\n",
        "\n",
        "    def current_token(self) -> Optional[str]:\n",
        "        return (\n",
        "            self.tokens[self.current_index]\n",
        "            if self.current_index < self.tokens_len\n",
        "            else None\n",
        "        )\n",
        "\n",
        "    def consume_token(self) -> Optional[str]:\n",
        "        current_token = self.current_token()\n",
        "        self.current_index += 1\n",
        "        return current_token\n",
        "\n",
        "    def is_current_token(self, token: str) -> bool:\n",
        "        current = self.current_token()\n",
        "        return current == token\n",
        "\n",
        "    def set_marker(self) -> None:\n",
        "        \"\"\"Sets the marker to the current position\"\"\"\n",
        "        self.marker = self.current_index\n",
        "\n",
        "    def from_marker_to(self, n: int) -> List[str]:\n",
        "        \"\"\"Returns the list of tokens from the marked up to n tokens forward\"\"\"\n",
        "        if n < 0:\n",
        "            raise ValueError(f\"n must be positive but is {n}\")\n",
        "        return self.tokens[self.marker : self.marker + n + 1]\n",
        "\n",
        "    def from_marker_to_current(self) -> List[str]:\n",
        "        \"\"\"Returns the list of tokens from the marked token to the current token\"\"\"\n",
        "        return self.tokens[self.marker : self.current_index + 1]\n",
        "\n",
        "\n",
        "class State(Enum):\n",
        "    Start = \"Start\"\n",
        "    End = \"End\"\n",
        "    Error = \"Error\"\n",
        "    FunctionName = \"FunctionName\"\n",
        "    FunctionGeneric = \"FunctionGeneric\"\n",
        "    Parameters = \"Parameters\"\n",
        "    ParseParameter = \"ParseParameter\"\n",
        "    ReturnType = \"ReturnType\"\n",
        "    ParseReturnType = \"ParseReturnType\"\n",
        "    FunctionBlock = \"FunctionBlock\"\n",
        "    ParseBlockContent = \"ParseBlockContent\"\n",
        "\n",
        "    def is_end_state(self, state: \"State\") -> bool:\n",
        "        return [State.End, State.Error] in state\n",
        "\n",
        "\n",
        "class FunctionParser:\n",
        "    def __init__(self, file, error_file=sys.stderr):\n",
        "        self.tokens = file\n",
        "        self.functions: List[FunctionData] = []\n",
        "        self.error_file = error_file\n",
        "\n",
        "    def __invalid_function_name(self, name: str) -> bool:\n",
        "        invalid_chars_in_name = set(\"{}()<>\")\n",
        "        return any((c in invalid_chars_in_name) for c in name)\n",
        "\n",
        "    def __build_err_msg_for_function(\n",
        "        self, title: str, tokenizer: Tokenizer, fn_data: FunctionData\n",
        "    ) -> None:\n",
        "        self.error_msg = f\"\"\"\n",
        "            {title}\n",
        "            raw: {\" \".join(tokenizer.from_marker_to(150))}\n",
        "            current: {\" \".join(tokenizer.from_marker_to_current())}\n",
        "            name: {fn_data.name}\n",
        "            params: {fn_data.params}\n",
        "            block: {fn_data.block}\n",
        "            ====================================================================================================================\n",
        "            \"\"\"\n",
        "\n",
        "    def parse(self) -> List[FunctionData]:\n",
        "        tokenizer = Tokenizer(self.tokens)\n",
        "        state = State.Start\n",
        "        current_function = FunctionData(None, [], [])\n",
        "        self.block_state = BlockParsingData()\n",
        "\n",
        "        while tokenizer.current_token() is not None and state is not State.Error:\n",
        "            if state == State.Start:\n",
        "                token = tokenizer.consume_token()\n",
        "                if token == \"function\":\n",
        "                    tokenizer.set_marker()\n",
        "                    state = State.FunctionName\n",
        "\n",
        "            elif state == State.FunctionName:\n",
        "                function_name = tokenizer.current_token()\n",
        "                if function_name == '*': # Only god knows why this is an error\n",
        "                    current_function = None\n",
        "                    state = State.End\n",
        "                if self.__invalid_function_name(function_name):\n",
        "                    function_name = \"anonymous\"\n",
        "                else:\n",
        "                    tokenizer.consume_token()\n",
        "                state = (\n",
        "                    State.FunctionGeneric\n",
        "                    if tokenizer.is_current_token(\"<\")\n",
        "                    else State.Parameters\n",
        "                )\n",
        "                # Clear current function data to load new\n",
        "                current_function.name = function_name\n",
        "                current_function.params.clear()\n",
        "                current_function.block.clear()\n",
        "\n",
        "            elif state == State.FunctionGeneric:\n",
        "                # Ignore function type declaration\n",
        "                while not tokenizer.is_current_token(\">\"):\n",
        "                    tokenizer.consume_token()\n",
        "                tokenizer.consume_token()\n",
        "                state = State.Parameters\n",
        "\n",
        "            elif state == State.Parameters:\n",
        "                if not tokenizer.is_current_token(\"(\"):\n",
        "                    self.error_msg = self.__build_err_msg_for_function(\n",
        "                        \"Unrecongnized token for Parameters\",\n",
        "                        tokenizer,\n",
        "                        current_function,\n",
        "                    )\n",
        "                    state = State.Error\n",
        "                    continue\n",
        "                self.params_state = ParametersParsingData()\n",
        "                state = State.ParseParameter\n",
        "\n",
        "            elif state == State.ParseParameter:\n",
        "                token: str = tokenizer.consume_token()\n",
        "\n",
        "                if token == \"(\":\n",
        "                    self.params_state.parenthesis.append(token)\n",
        "                elif token == \")\":\n",
        "                    if len(self.params_state.parenthesis) == 0:\n",
        "                        self.__build_err_msg_for_function(\n",
        "                            \"Bad Parenthesis\", tokenizer, current_function\n",
        "                        )\n",
        "                        state = State.Error\n",
        "                        continue\n",
        "                    try:\n",
        "                      self.params_state.parenthesis.pop()\n",
        "                    except:\n",
        "                      current_function = None\n",
        "                      state = State.End\n",
        "                    if len(self.params_state.parenthesis) == 0:\n",
        "                        state = State.FunctionBlock\n",
        "                        continue\n",
        "\n",
        "                elif self.params_state.is_type_def:\n",
        "                    if token == \"{\":\n",
        "                        self.params_state.braces.append(token)\n",
        "                    elif token == \"}\":\n",
        "                      try:\n",
        "                        self.params_state.braces.pop()\n",
        "                      except:\n",
        "                        current_function = None\n",
        "                        state = State.End\n",
        "                    elif token == \"<\":\n",
        "                        self.params_state.sharp_braces.append(token)\n",
        "                    elif token == \">\":\n",
        "                        try:\n",
        "                          self.params_state.sharp_braces.pop()\n",
        "                        except:\n",
        "                          current_function = None\n",
        "                          state = State.End\n",
        "                    elif (\n",
        "                        token == \",\"\n",
        "                        and len(self.params_state.braces) == 0\n",
        "                        and len(self.params_state.sharp_braces) == 0\n",
        "                    ):\n",
        "                        self.params_state.is_type_def = False\n",
        "                else:\n",
        "                    if token == \":\":\n",
        "                        self.params_state.is_type_def = True\n",
        "                    elif token != \",\" and token != \"{\" and token != \"}\":\n",
        "                        current_function.params.append(token)\n",
        "\n",
        "            elif state == State.ReturnType:\n",
        "                self.return_type_data = ReturnTypeData()\n",
        "                if tokenizer.is_current_token(\"{\"):\n",
        "                    self.return_type_data.containers += 1\n",
        "                    tokenizer.consume_token()\n",
        "                state = State.ParseReturnType\n",
        "\n",
        "            elif state == State.ParseReturnType:\n",
        "                while True:\n",
        "                    token = tokenizer.current_token()\n",
        "                    if token == \"{\":\n",
        "                        if self.return_type_data.containers == 0:\n",
        "                            break\n",
        "                        self.return_type_data.containers += 1\n",
        "                    elif token in [\"(\", \"[\", \"<\"]:\n",
        "                        self.return_type_data.containers += 1\n",
        "                    elif token in [\")\", \"]\", \">\", \"}\"]:\n",
        "                        self.return_type_data.containers -= 1\n",
        "\n",
        "                    tokenizer.consume_token()\n",
        "\n",
        "                state = State.FunctionBlock\n",
        "\n",
        "            elif state == State.FunctionBlock:\n",
        "                if tokenizer.is_current_token(\";\"):\n",
        "                    # Is a function call\n",
        "                    current_function = None\n",
        "                    state = State.End\n",
        "                elif tokenizer.is_current_token(\":\"):\n",
        "                    tokenizer.consume_token()\n",
        "                    state = State.ReturnType\n",
        "                elif not tokenizer.is_current_token(\"{\"):\n",
        "                    self.__build_err_msg_for_function(\n",
        "                        \"Unrecognized token for Function Block\",\n",
        "                        tokenizer,\n",
        "                        current_function,\n",
        "                    )\n",
        "                    state = State.Error\n",
        "                else:\n",
        "                    self.block_state = BlockParsingData()\n",
        "                    state = State.ParseBlockContent\n",
        "\n",
        "            elif state == State.ParseBlockContent:\n",
        "                token = tokenizer.consume_token()\n",
        "                current_function.block.append(token)\n",
        "\n",
        "                if token == \"{\" or token.startswith(\"{\"):\n",
        "                    self.block_state.braces.append(token)\n",
        "                elif token == \"}\" or token.endswith(\"}\"):\n",
        "                    if len(self.block_state.braces) == 0:\n",
        "                        self.__build_err_msg_for_function(\n",
        "                            \"Bad Program\", tokenizer, current_function\n",
        "                        )\n",
        "                        state = State.Error\n",
        "                        continue\n",
        "                    try:\n",
        "                      self.block_state.braces.pop()\n",
        "                    except:\n",
        "                      current_function = None\n",
        "                      state = State.End\n",
        "                    if len(self.block_state.braces) == 0:\n",
        "                        state = State.End\n",
        "            elif state == State.End:\n",
        "                if current_function is not None:\n",
        "                  self.functions.append(current_function)\n",
        "                else:\n",
        "                  current_function = FunctionData(None, [], [])\n",
        "                state = State.Start\n",
        "\n",
        "        if state not in [State.Start, State.End, State.Error]:\n",
        "            self.__build_err_msg_for_function(\n",
        "                \"Reached end of file wihtout completion\", tokenizer, current_function\n",
        "            )\n",
        "            state = State.Error\n",
        "        if state is State.Error:\n",
        "            if self.error_msg is None:\n",
        "                self.__build_err_msg_for_function(\n",
        "                    \"Unknown error\", tokenizer, current_function\n",
        "                )\n",
        "\n",
        "            print(self.error_msg, file=self.error_file)\n",
        "\n",
        "        return self.functions\n",
        "\n",
        "\n",
        "def extract_functions(\n",
        "    tokens_list: Iterable[List[str]],\n",
        ") -> List[FunctionData]:\n",
        "    functions: List[FunctionData] = []\n",
        "    error_file = open(\"./errors.log\", \"a+\")\n",
        "    error_file.truncate(0)\n",
        "    for tokens in tokens_list:\n",
        "        parser = FunctionParser(tokens, error_file)\n",
        "        functions.extend(parser.parse())\n",
        "    error_file.close()\n",
        "    return [fn for fn in functions if fn is not None]\n"
      ],
      "metadata": {
        "id": "sxA0MdjXpqkR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manipulate Data"
      ],
      "metadata": {
        "id": "-fsJkNQO-Vy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_functions = extract_functions(files)\n"
      ],
      "metadata": {
        "id": "7Mb_cXhz-cV6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdBW12XJ9tqy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Commutate function names to get bad results"
      ],
      "metadata": {
        "id": "VujT4FCv7AY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rnd\n",
        "\n",
        "def switch_function_name(fn1: FunctionData, fn2: FunctionData) -> FunctionData:\n",
        "  return FunctionData(fn2.name, fn1.params, fn1.block)\n",
        "\n",
        "good_fns_amount = len(good_functions)\n",
        "\n",
        "bad_functions: List[FunctionData] = []\n",
        "comparisons = []\n",
        "\n",
        "for i, fn in enumerate(good_functions):\n",
        "  other_index = i\n",
        "  while other_index == i:\n",
        "    other_index = rnd.randint(0, good_fns_amount - 1)\n",
        "\n",
        "  other_fn = good_functions[other_index]\n",
        "\n",
        "  comparisons.append((fn.name, other_fn.name))\n",
        "  bad_functions.append(switch_function_name(fn, other_fn))\n",
        "\n",
        "# Just to check that the switchs are well made\n",
        "DataFrame(comparisons).to_csv('./comparisons.csv')\n",
        "\n",
        "functions = good_functions + bad_functions\n"
      ],
      "metadata": {
        "id": "diRtk7UM7G8e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Final Corpus"
      ],
      "metadata": {
        "id": "QbO5eoYEAKTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q gensim\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader\n",
        "import re\n",
        "\n",
        "def parse_cammel_case(fn_name: str) -> List[str]:\n",
        "  return re.findall(\"[a-z]+|[A-Z][a-z]*\", fn_name)"
      ],
      "metadata": {
        "id": "IuuBB5P9AM5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}